# -*- coding: utf-8 -*-
"""Assignment_4_GURJUSSINGH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JQywvTZYq_F-hXy04JGXmfU41GDrzt5S

# **APPENDIX**
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout 
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam

from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

import tensorflow as tf
from sklearn.decomposition import PCA

import cv2
import os

import numpy as np

from sklearn.model_selection import train_test_split

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import models, layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dropout, Flatten, Input, Dense

# Load the Drive helper and mount
from google.colab import drive
drive.mount('/content/drive')

import matplotlib.image as mpimg
directory=os.listdir('/content/drive/MyDrive/COVID/train/')
for each in directory:
    plt.figure()
    currentFolder = '/content/drive/MyDrive/COVID/train/' + each
    for i, file in enumerate(os.listdir(currentFolder)[0:5]):
        fullpath = currentFolder  + "/" + file
        print(fullpath)
        img=mpimg.imread(fullpath)
        plt.subplot(2, 3, i+1)
        plt.imshow(img)

import matplotlib.image as mpimg
directory=os.listdir('/content/drive/MyDrive/COVID/train/')
for each in directory:
    plt.figure()
    currentFolder = '/content/drive/MyDrive/COVID/train/' + each
    for i, file in enumerate(os.listdir(currentFolder)[0:5]):
        fullpath = currentFolder  + "/" + file
        print(fullpath)
        img=mpimg.imread(fullpath)
        plt.subplot(2, 3, i+1)
        plt.imshow(img)

labels = ['Covid Negative', 'Covid Positive']
img_size = 64
def get_data(data_dir):
    data = [] 
    for label in labels: 
        path = os.path.join(data_dir, label)
        class_num = labels.index(label)
        for img in os.listdir(path):
            try:
                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format
                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size
                data.append([resized_arr, class_num])
            except Exception as e:
                print(e)
    return np.array(data)

train = get_data('/content/drive/MyDrive/COVID/train/')
test = get_data('/content/drive/MyDrive/COVID/test/')

path = '/content/drive/MyDrive/COVID/train/Covid Positive'
path1 = '/content/drive/MyDrive/COVID/test/Covid Positive'
path2 = '/content/drive/MyDrive/COVID/train/Covid Negative'
path3 = '/content/drive/MyDrive/COVID/test/Covid Negative'
covidpositives = len([f for f in os.listdir(path)if os.path.isfile(os.path.join(path, f))]) + len([f for f in os.listdir(path1)if os.path.isfile(os.path.join(path1, f))])
covidnegatives = len([f for f in os.listdir(path2)if os.path.isfile(os.path.join(path2, f))]) + len([f for f in os.listdir(path3)if os.path.isfile(os.path.join(path3, f))])

Cats = ['Covid Positive', 'Covid Negative']
y_pos = np.arange(len(Cats))
barlist = plt.bar(y_pos,[covidpositives, covidnegatives], align='center', alpha=0.5)
barlist[0].set_color('g')
barlist[1].set_color('r')
plt.xticks(y_pos,['Covid Positive', 'Covid Negative'])
plt.ylabel('Number of Cases')
plt.title('Coronavirus Cases and Categories')

plt.show()

path = '/content/drive/MyDrive/COVID/test/Covid Negative'
path1 = '/content/drive/MyDrive/COVID/test/Covid Positive'
path2 = '/content/drive/MyDrive/COVID/train/Covid Negative'
path3 = '/content/drive/MyDrive/COVID/train/Covid Positive'
Test = len([f for f in os.listdir(path)if os.path.isfile(os.path.join(path, f))]) + len([f for f in os.listdir(path1)if os.path.isfile(os.path.join(path1, f))])
Train = len([f for f in os.listdir(path2)if os.path.isfile(os.path.join(path2, f))]) + len([f for f in os.listdir(path3)if os.path.isfile(os.path.join(path3, f))])
# Data to plot
labels = 'Training', 'Testing'
sizes = [Train, Test]
colors = ['gold', 'yellowgreen']
explode = (0, 0)  # explode 1st slice

# Plot
plt.pie(sizes, explode=explode, labels=labels, colors=colors,
autopct='%1.1f%%', shadow=True, startangle=140)

plt.axis('equal')
plt.show()

plt.figure(figsize = (5,5))
plt.imshow(train[1][0])
plt.title('Training Set: Covid Negative')

plt.figure(figsize = (5,5))
plt.imshow(train[-1][0])
plt.title('Training Set: Covid Positive')

x_train = []
y_train = []
x_test = []
y_test = []

for feature, label in train:
  x_train.append(feature)
  y_train.append(label)

for feature, label in test:
  x_test.append(feature)
  y_test.append(label)

# Normalize the data
x_train = np.array(x_train) / 255
x_test = np.array(x_test) / 255

x_train.reshape(-1, img_size, img_size, 1)
y_train = np.array(y_train)

x_test.reshape(-1, img_size, img_size, 1)
y_test = np.array(y_test)

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.2, # Randomly zoom image 
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip = True,  # randomly flip images
        vertical_flip=True)  # randomly flip images


datagen.fit(x_train)

X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

"""# Experiment 1"""

model = models.Sequential()
model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model.add(layers.MaxPooling2D((2, 2),strides=2))
model.add(layers.Flatten())
model.add(layers.Dense(units=32, activation=tf.nn.relu))
model.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = model.fit(X_train,
#                     y_train,
#                     validation_data = (X_val, y_val),
#                     epochs=20,
#                     batch_size=512                                                                                                         
#                    )

import numpy as np


loss, accuracy = model.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 2"""

model2 = models.Sequential()
model2.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model2.add(layers.MaxPooling2D((2, 2),strides=2))
model2.add(layers.Flatten())
model2.add(layers.Dense(units=64, activation=tf.nn.relu))
model2.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model2.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history2 = model2.fit(X_train,
#                     y_train,
#                     validation_data = (X_val, y_val),
#                     epochs=20,
#                     batch_size=512                                                                                                         
#                    )

loss, accuracy = model2.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 3"""

model3 = models.Sequential()
model3.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model3.add(layers.MaxPooling2D((2, 2),strides=2))
model3.add(layers.Flatten())
model3.add(layers.Dense(units=128, activation=tf.nn.relu))
model3.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model3.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history3 = model3.fit(X_train,
#                     y_train,
#                     validation_data = (X_val, y_val),
#                     epochs=20,
#                     batch_size=512                                                                                                         
#                    )

loss, accuracy = model3.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 4"""

model4 = models.Sequential()
model4.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model4.add(layers.MaxPooling2D((2, 2),strides=2))
model4.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model4.add(layers.MaxPooling2D((2, 2),strides=2))
model4.add(layers.Flatten())4
model4.add(layers.Dense(units=256, activation=tf.nn.relu))
model4.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model4.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history4 = model4.fit(X_train,
#                     y_train,
#                     validation_data = (X_val, y_val),
#                     epochs=20,
#                     batch_size=512                                                                                                         
#                    )

loss, accuracy = model4.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 5"""

model5 = models.Sequential()
model5.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model5.add(layers.MaxPooling2D((2, 2),strides=2))
model5.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model5.add(layers.MaxPooling2D((2, 2),strides=2))
model5.add(layers.Flatten())
model5.add(layers.Dense(units=256, activation=tf.nn.relu))
model5.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model5.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history5 = model5.fit(X_train,
#                     y_train,
#                     validation_data = (X_val, y_val),
#                     epochs=20,
#                     batch_size=512                                                                                                         
#                    )

loss, accuracy = model5.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 6"""

model6 = models.Sequential()
model6.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model6.add(layers.MaxPooling2D((2, 2),strides=2))
model6.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model6.add(layers.MaxPooling2D((2, 2),strides=2))
model6.add(layers.Flatten())
model6.add(layers.Dense(units=256, activation=tf.nn.relu))
model6.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model6.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history6 = model6.fit(X_train,
#                     y_train,
#                     validation_data = (X_val, y_val),
#                     epochs=20,
#                     batch_size=512                                                                                                         
#                    )

loss, accuracy = model6.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 7"""

model7 = models.Sequential()
model7.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model7.add(layers.MaxPooling2D((2, 2),strides=2))
model7.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model7.add(layers.MaxPooling2D((2, 2),strides=2))
model7.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model7.add(layers.MaxPooling2D((2, 2),strides=2))
model7.add(layers.Flatten())
model7.add(layers.Dense(units=256, activation=tf.nn.relu))
model7.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model7.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history7 = model7.fit(X_train,
#                     y_train,
#                     validation_data = (X_val, y_val),
#                     epochs=20,
#                     batch_size=512                                                                                                         
#                    )

loss, accuracy = model7.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 8"""

from keras.layers import AveragePooling2D
model8 = models.Sequential()
model8.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model8.add(layers.AveragePooling2D((2, 2),strides=2))
model8.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model8.add(layers.AveragePooling2D((2, 2),strides=2))
model8.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model8.add(layers.AveragePooling2D((2, 2),strides=2))
model8.add(layers.Flatten())
model8.add(layers.Dense(units=256, activation=tf.nn.relu))
model8.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model8.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
#  %%time
history8 = model8.fit(X_train,
                    y_train,
                    validation_data = (X_val, y_val),
                    epochs=20,
                    batch_size=512                                                                                                         
                   )

loss, accuracy = model8.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 9"""

model9 = models.Sequential()
model9.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model9.add(layers.MaxPooling2D((2, 2),strides=2))
model9.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model9.add(layers.MaxPooling2D((2, 2),strides=2))
model9.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model9.add(layers.MaxPooling2D((2, 2),strides=2))
model9.add(layers.Flatten())
model9.add(layers.Dense(units=256, activation=tf.nn.relu))
model9.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model9.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
#  %%time
history9 = model9.fit(X_train,
                    y_train,
                    validation_data = (X_val, y_val),
                    epochs=20,
                    batch_size=512                                                                                                         
                   )

loss, accuracy = model9.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

y_pred = (model9.predict(X_train) > 0.5).astype("int32")
confusion_matrix(y_train, y_pred)

f1_score(y_train, y_pred, average='macro')

recall_score(y_train, y_pred, average='macro')

precision_score(y_train, y_pred, average='macro')

y_pred = (model9.predict(x_test) > 0.5).astype("int32")
confusion_matrix(y_test, y_pred)

f1_score(y_test, y_pred, average='macro')

recall_score(y_test, y_pred, average='macro')

precision_score(y_test, y_pred, average='macro')

from keras.preprocessing import image
import numpy as np



img_tensor = image.img_to_array(x_test[2])
img_tensor = np.expand_dims(img_tensor, axis=0)
# Remember that the model was trained on inputs
# that were preprocessed in the following way:
img_tensor /= 255.


from keras import models

# Extracts the outputs of the top 8 layers:
layer_outputs = [layer.output for layer in model9.layers[:2]]
# Creates a model that will return these outputs, given the model input:
activation_model = models.Model(inputs=model9.input, outputs=layer_outputs)

# This will return a list of 5 Numpy arrays:
# one array per layer activation
activations = activation_model.predict(img_tensor)

first_layer_activation = activations[-1]
print(first_layer_activation.shape)


import keras

# These are the names of the layers, so can have them as part of our plot
layer_names = []
for layer in model9.layers[:8]:
    layer_names.append(layer.name)

images_per_row = 16

# Now let's display our feature maps
for layer_name, layer_activation in zip(layer_names, activations):
    # This is the number of features in the feature map
    n_features = layer_activation.shape[-1]

    # The feature map has shape (1, size, size, n_features)
    size = layer_activation.shape[1]

    # We will tile the activation channels in this matrix
    n_cols = n_features // images_per_row
    display_grid = np.zeros((size * n_cols, images_per_row * size))

    # We'll tile each filter into this big horizontal grid
    for col in range(n_cols):
        for row in range(images_per_row):
            channel_image = layer_activation[0,
                                             :, :,
                                             col * images_per_row + row]
            # Post-process the feature to make it visually palatable
            channel_image -= channel_image.mean()
            channel_image /= channel_image.std()
            channel_image *= 64
            channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype('uint8')
            display_grid[col * size : (col + 1) * size,
                         row * size : (row + 1) * size] = channel_image

    # Display the grid
    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1],
                        scale * display_grid.shape[0]))
    plt.title(layer_name)
    plt.grid(False)
    plt.imshow(display_grid, aspect='auto', cmap='viridis')
    
plt.show()

pred_classes = (model9.predict(x_test) > 0.5).astype("int32").ravel()

pred_classes

layer_outputs = [layer.output for layer in model9.layers]
activation_model = models.Model(inputs=model9.input, outputs=layer_outputs)
layer_outputs

# Get the outputs of all the hidden nodes for each of the 60000 training images
activations = activation_model.predict(x_test) 
hidden_layer_activation = activations[7]
output_layer_activations = activations[8]
hidden_layer_activation.shape   #  each of the 128 hidden nodes has one activation value per training image

#Get the dataframe of all the node values
activation_data = {'pred_class':pred_classes[0:328]}
for k in range(0,256): 
    activation_data[f"act_val_{k}"] = hidden_layer_activation[:,k]


activation_df = pd.DataFrame(activation_data)
activation_df.head()

# Separating out the features
features = [*activation_data][1:] # ['act_val_0', 'act_val_1',...]
x = activation_df.loc[:, features].values 

pca = PCA(n_components=3)
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents
             , columns = ['pca-one', 'pca-two', 'pca-three'])
principalDf.head()

activation_pca_df = pd.concat([principalDf, activation_df[['pred_class']]], axis = 1)
activation_pca_df.head()

N=10000
activation_df_subset = activation_df.iloc[:N].copy()
activation_df_subset.shape

data_subset = activation_df_subset[features].values
data_subset.shape

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
tsne_results = tsne.fit_transform(data_subset)

activation_df_subset['tsne-2d-one'] = tsne_results[:,0]
activation_df_subset['tsne-2d-two'] = tsne_results[:,1]

plt.figure(figsize=(16,10))
sns.scatterplot(
    x="tsne-2d-one", y="tsne-2d-two",
    hue="pred_class",
    palette=sns.color_palette(n_colors = 2),
    data=activation_df_subset,
    legend="full",
    alpha = 1
)

"""# Experiment 10"""

model10 = models.Sequential()
model10.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model10.add(layers.MaxPooling2D((2, 2),strides=2))
model10.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model10.add(layers.MaxPooling2D((2, 2),strides=2))
model10.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model10.add(layers.MaxPooling2D((2, 2),strides=2))
model10.add(layers.Flatten())
model10.add(layers.Dense(units=256, activation=tf.nn.relu))
model10.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model10.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
#  %%time
history10 = model10.fit(X_train,
                    y_train,
                    validation_data = (X_val, y_val),
                    epochs=20,
                    batch_size=512                                                                                                         
                   )

loss, accuracy = model10.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 11"""

model11 = models.Sequential()
model11.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model11.add(layers.MaxPooling2D((2, 2),strides=2))
model11.add(layers.Dropout(.2))
model11.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model11.add(layers.MaxPooling2D((2, 2),strides=2))
model11.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model11.add(layers.MaxPooling2D((2, 2),strides=2))
model11.add(layers.Flatten())
model11.add(layers.Dense(units=256, activation=tf.nn.relu))
model11.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model11.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
#  %%time
history11 = model11.fit(X_train,
                    y_train,
                    validation_data = (X_val, y_val),
                    epochs=20,
                    batch_size=512                                                                                                         
                   )

loss, accuracy = model11.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 12"""

model12 = models.Sequential()
model12.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model12.add(layers.MaxPooling2D((2, 2),strides=2))
model12.add(layers.Dropout(.2))
model12.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model12.add(layers.MaxPooling2D((2, 2),strides=2))
model11.add(layers.Dropout(.2))
model12.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model12.add(layers.MaxPooling2D((2, 2),strides=2))
model12.add(layers.Flatten())
model12.add(layers.Dense(units=256, activation=tf.nn.relu))
model12.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model12.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
#  %%time
history12 = model12.fit(X_train,
                    y_train,
                    validation_data = (X_val, y_val),
                    epochs=20,
                    batch_size=512                                                                                                         
                   )

loss, accuracy = model12.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 13"""

model13 = models.Sequential()
model13.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model13.add(layers.MaxPooling2D((2, 2),strides=2))
model13.add(layers.Dropout(.2))
model13.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model13.add(layers.MaxPooling2D((2, 2),strides=2))
model13.add(layers.Dropout(.2))
model13.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model13.add(layers.MaxPooling2D((2, 2),strides=2))
model13.add(layers.Dropout(.2))
model13.add(layers.Flatten())
model13.add(layers.Dense(units=256, activation=tf.nn.relu))
model13.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model13.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
#  %%time
history13 = model13.fit(X_train,
                    y_train,
                    validation_data = (X_val, y_val),
                    epochs=20,
                    batch_size=512                                                                                                         
                   )

loss, accuracy = model13.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 14"""

model14 = models.Sequential()
model14.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model14.add(layers.MaxPooling2D((2, 2),strides=2))
model14.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model14.add(layers.MaxPooling2D((2, 2),strides=2))
model14.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model14.add(layers.MaxPooling2D((2, 2),strides=2))
model14.add(layers.Flatten())
model14.add(layers.Dense(units=256, activation=tf.nn.relu))
model14.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model14.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
#  %%time
history14 = model14.fit(X_train,
                    y_train,
                    validation_data = (X_val, y_val),
                    epochs=20,
                    batch_size=512,                                                                                                         
                   callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=1)])

loss, accuracy = model14.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 15"""

model15 = models.Sequential()
model15.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model15.add(layers.MaxPooling2D((2, 2),strides=2))
model15.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model15.add(layers.MaxPooling2D((2, 2),strides=2))
model15.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model15.add(layers.MaxPooling2D((2, 2),strides=2))
model15.add(layers.Flatten())
model15.add(layers.Dense(units=256, activation=tf.nn.relu))
model15.add(layers.Dense(units=1, activation=tf.nn.sigmoid))

model15.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
#  %%time
history15 = model15.fit(X_train,
                    y_train,
                    validation_data = (X_val, y_val),
                    epochs=20,
                    batch_size=512,                                                                                                         
                   callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)])

loss, accuracy = model15.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 16"""

model16 = models.Sequential()
model16.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model16.add(layers.MaxPooling2D((2, 2),strides=2))
model16.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model16.add(layers.MaxPooling2D((2, 2),strides=2))
model16.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model16.add(layers.MaxPooling2D((2, 2),strides=2))
model16.add(layers.Flatten())
model16.add(layers.Dense(units=256, activation=tf.nn.relu))
model16.add(layers.Dense(units=1, activation=tf.nn.sigmoid, kernel_regularizer='l1'))

model16.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
#  %%time
history16 = model16.fit(X_train,
                    y_train,
                    validation_data = (X_val, y_val),
                    epochs=20,
                    batch_size=512)

loss, accuracy = model16.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)

"""# Experiment 17"""

model17 = models.Sequential()
model17.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, input_shape=(64, 64, 3)))
model17.add(layers.MaxPooling2D((2, 2),strides=2))
model17.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model17.add(layers.MaxPooling2D((2, 2),strides=2))
model17.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))
model17.add(layers.MaxPooling2D((2, 2),strides=2))
model17.add(layers.Flatten())
model17.add(layers.Dense(units=256, activation=tf.nn.relu))
model17.add(layers.Dense(units=1, activation=tf.nn.sigmoid, kernel_regularizer='l2'))

model17.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
#  %%time
history17 = model17.fit(X_train,
                    y_train,
                    validation_data = (X_val, y_val),
                    epochs=20,
                    batch_size=512)

loss, accuracy = model17.evaluate(x_test, y_test)
print('test set accuracy: ', accuracy * 100)