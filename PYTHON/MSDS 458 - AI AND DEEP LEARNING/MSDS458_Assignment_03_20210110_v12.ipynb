{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MSDS458_Assignment_03_20210110_v12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilN0Id3L1och"
      },
      "source": [
        "\n",
        "# **Appendix**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfh9kVB-rpjo"
      },
      "source": [
        "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/NorthwesternHeader.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRAx1f1ir1St"
      },
      "source": [
        "## MSDS458 Research Assignment 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQnyR1_aFk-t"
      },
      "source": [
        "### Analyze AG_NEWS_SUBSET Data <br>\n",
        "\n",
        "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity.<br> \n",
        "\n",
        "For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html<br> \n",
        "\n",
        "\n",
        "The AG's news topic classification dataset is constructed by choosing 4 largest classes (**World**, **Sports**, **Business**, and **Sci/Tech**) from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.<br>\n",
        "\n",
        "Homepage: https://arxiv.org/abs/1509.01626<br>\n",
        "\n",
        "Source code: tfds.text.AGNewsSubset\n",
        "\n",
        "Versions:\n",
        "\n",
        "1.0.0 (default): No release notes.\n",
        "Download size: 11.24 MiB\n",
        "\n",
        "Dataset size: 35.79 MiB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGBlk-2-NaCn"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    <b>More Technical</b>: Throughout the notebook. This types of boxes provide more technical details and extra references about what you are seeing. They contain helpful tips, but you can safely skip them the first time you run through the code.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-Jjr3rNr5PR"
      },
      "source": [
        "## Import packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbrX9qUZvfs1"
      },
      "source": [
        "import datetime\n",
        "from packaging import version\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0H8HuzHrWwN"
      },
      "source": [
        "%matplotlib inline\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2iqmNYKxw3A"
      },
      "source": [
        "### Create a Helper Function to Plot Graphs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg13oDZNxxMp"
      },
      "source": [
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKucNfkrsqrP"
      },
      "source": [
        "### Verify TensorFlow Version and Keras Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_BoGg9JrWzj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e35f71-f073-4b0a-98f4-ae4a592889eb"
      },
      "source": [
        "print(\"This notebook requires TensorFlow 2.0 or above\")\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "assert version.parse(tf.__version__).release[0] >=2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This notebook requires TensorFlow 2.0 or above\n",
            "TensorFlow version:  2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_bLWHP4rW3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59db4412-ab3c-4fd4-e4c2-0a6e63756c85"
      },
      "source": [
        "print(\"Keras version: \", keras.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keras version:  2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3-S2vDGs_ye"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    <b>Suppress warning messages</b></div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh3EX5eBrW6R"
      },
      "source": [
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mFyXjJmtJ3E"
      },
      "source": [
        "### Mount Google Drive to Colab Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXm3H81YrW8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb80796-d605-4553-ddf3-5f984ce8197a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6s4xhh5ysp6"
      },
      "source": [
        "###  TensorFlow Datasets Information<br>\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "    <b> ag_news_subset</b><br>\n",
        "    See https://www.tensorflow.org/datasets/catalog/ag_news_subset\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL3ewSkAF4yF"
      },
      "source": [
        "Get all the words in the documents (as well as the number of words in each document) by using the encoder to get the indices associated with each token and then translating the indices to tokens. But first we need to get the \"unpadded\" new articles so that we can get their length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNj2HSxNtTNS",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d66a37-b7e1-482c-b871-16f33f8285b5"
      },
      "source": [
        "#register  ag_news_subset so that tfds.load doesn't generate a checksum (mismatch) error\n",
        "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=ag_news_subset\n",
        "\n",
        "# https://www.tensorflow.org/datasets/splits\n",
        "# The full `train` and `test` splits, interleaved together.\n",
        "ri = tfds.core.ReadInstruction('train') + tfds.core.ReadInstruction('test')\n",
        "dataset_all, info = tfds.load('ag_news_subset', with_info=True,  split=ri, as_supervised=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-20 19:51:31.137476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "I0220 19:51:34.107089 140320970397568 download_and_prepare.py:200] Running download_and_prepare for dataset(s):\n",
            "ag_news_subset\n",
            "2021-02-20 19:51:34.123344: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2021-02-20 19:51:34.197100: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2021-02-20 19:51:34.259667: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "I0220 19:51:34.321993 140320970397568 dataset_info.py:434] Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: ag_news_subset/1.0.0\n",
            "2021-02-20 19:51:34.335444: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2021-02-20 19:51:34.610769: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "I0220 19:51:34.813735 140320970397568 dataset_info.py:361] Load dataset info from /tmp/tmp5brljkm8tfds\n",
            "I0220 19:51:34.815985 140320970397568 download_and_prepare.py:138] download_and_prepare for dataset ag_news_subset/1.0.0...\n",
            "I0220 19:51:34.816476 140320970397568 dataset_builder.py:357] Generating dataset ag_news_subset (/root/tensorflow_datasets/ag_news_subset/1.0.0)\n",
            "\u001b[1mDownloading and preparing dataset ag_news_subset/1.0.0 (download: 11.24 MiB, generated: 35.79 MiB, total: 47.03 MiB) to /root/tensorflow_datasets/ag_news_subset/1.0.0...\u001b[0m\n",
            "2021-02-20 19:51:35.002639: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2021-02-20 19:51:35.071403: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[AI0220 19:51:35.145200 140320970397568 download_manager.py:476] Downloading https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms into /root/tensorflow_datasets/downloads/ucexport_download_id_0Bz8a_Dbh9QhbUDNpeUdjb0wxj4g1umFAV8OV-uDwxSJR0LdxO_k1jxMuFWwAfNX9jos.tmp.9c8d78f6321b45ceaaf7007dec782779...\n",
            "Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:05, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:05, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 1 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 2 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 3 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 4 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 5 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 6 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 7 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 8 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 9 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 10 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 11 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:05<00:00,  5.59s/ url]\n",
            "Dl Size...: 11 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:05<00:00,  5.59s/ url]\n",
            "Dl Size...: 11 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Extraction completed...:   0% 0/1 [00:05<?, ? file/s]\u001b[A\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:05<00:00,  5.59s/ url]\n",
            "Dl Size...: 11 MiB [00:05,  5.49s/ MiB]\u001b[A\n",
            "\n",
            "Extraction completed...: 100% 1/1 [00:05<00:00,  5.96s/ file]\u001b[A\u001b[A\n",
            "Extraction completed...: 100% 1/1 [00:05<00:00,  5.96s/ file]\n",
            "\n",
            "Dl Size...: 11 MiB [00:05,  1.84 MiB/s]\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:05<00:00,  5.96s/ url]\n",
            "I0220 19:51:41.106563 140320970397568 dataset_builder.py:970] Generating split train\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteK5DYJN/ag_news_subset-train.tfrecord\n",
            " 69% 82535/120000 [00:00<34:05, 18.32 examples/s]  I0220 19:52:16.678695 140320970397568 tfrecords_writer.py:226] Done writing /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteK5DYJN/ag_news_subset-train.tfrecord. Shard lengths: [120000]\n",
            "I0220 19:52:16.695761 140320970397568 dataset_builder.py:970] Generating split test\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteK5DYJN/ag_news_subset-test.tfrecord\n",
            "  0% 0/7600 [00:00<?, ? examples/s]I0220 19:52:18.963265 140320970397568 tfrecords_writer.py:226] Done writing /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteK5DYJN/ag_news_subset-test.tfrecord. Shard lengths: [7600]\n",
            "I0220 19:52:18.964528 140320970397568 dataset_builder.py:412] Skipping computing stats for mode ComputeStatsMode.SKIP.\n",
            "\u001b[1mDataset ag_news_subset downloaded and prepared to /root/tensorflow_datasets/ag_news_subset/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "\u001b[1mname: \"ag_news_subset\"\n",
            "description: \"AG is a collection of more than 1 million news articles.\\nNews articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity.\\nComeToMyHead is an academic news search engine which has been running since July, 2004.\\nThe dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc),\\ninformation retrieval (ranking, search, etc), xml, data compression, data streaming,\\nand any other non-commercial activity.\\nFor more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\\n\\nThe AG\\'s news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above.\\nIt is used as a text classification benchmark in the following paper:\\nXiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\\n\\nThe AG\\'s news topic classification dataset is constructed by choosing 4 largest classes from the original corpus.\\nEach class contains 30,000 training samples and 1,900 testing samples.\\nThe total number of training samples is 120,000 and testing 7,600.\"\n",
            "citation: \"@misc{zhang2015characterlevel,\\n    title={Character-level Convolutional Networks for Text Classification},\\n    author={Xiang Zhang and Junbo Zhao and Yann LeCun},\\n    year={2015},\\n    eprint={1509.01626},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.LG}\\n}\"\n",
            "location {\n",
            "  urls: \"https://arxiv.org/abs/1509.01626\"\n",
            "}\n",
            "splits {\n",
            "  name: \"test\"\n",
            "  shard_lengths: 7600\n",
            "  num_bytes: 2226751\n",
            "}\n",
            "splits {\n",
            "  name: \"train\"\n",
            "  shard_lengths: 120000\n",
            "  num_bytes: 35301386\n",
            "}\n",
            "supervised_keys {\n",
            "  input: \"description\"\n",
            "  output: \"label\"\n",
            "}\n",
            "version: \"1.0.0\"\n",
            "download_size: 11784327\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzs2FWbQIf5m"
      },
      "source": [
        "###  Exploratory Analysis AG News Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7514-j4I4B-"
      },
      "source": [
        "**Get information about the ag_news_subset dataset. We combined the training and test data for a total of 127,600 news articles.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJNcnZbRtTQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d459d885-0227-4463-d5df-68ae2cc7b258"
      },
      "source": [
        "info"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='ag_news_subset',\n",
              "    version=1.0.0,\n",
              "    description='AG is a collection of more than 1 million news articles.\n",
              "News articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity.\n",
              "ComeToMyHead is an academic news search engine which has been running since July, 2004.\n",
              "The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc),\n",
              "information retrieval (ranking, search, etc), xml, data compression, data streaming,\n",
              "and any other non-commercial activity.\n",
              "For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n",
              "\n",
              "The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above.\n",
              "It is used as a text classification benchmark in the following paper:\n",
              "Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
              "\n",
              "The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus.\n",
              "Each class contains 30,000 training samples and 1,900 testing samples.\n",
              "The total number of training samples is 120,000 and testing 7,600.',\n",
              "    homepage='https://arxiv.org/abs/1509.01626',\n",
              "    features=FeaturesDict({\n",
              "        'description': Text(shape=(), dtype=tf.string),\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=4),\n",
              "        'title': Text(shape=(), dtype=tf.string),\n",
              "    }),\n",
              "    total_num_examples=127600,\n",
              "    splits={\n",
              "        'test': 7600,\n",
              "        'train': 120000,\n",
              "    },\n",
              "    supervised_keys=('description', 'label'),\n",
              "    citation=\"\"\"@misc{zhang2015characterlevel,\n",
              "        title={Character-level Convolutional Networks for Text Classification},\n",
              "        author={Xiang Zhang and Junbo Zhao and Yann LeCun},\n",
              "        year={2015},\n",
              "        eprint={1509.01626},\n",
              "        archivePrefix={arXiv},\n",
              "        primaryClass={cs.LG}\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6GaSzMRqrIR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "91e1bfb8-f5b6-43df-af5f-07cbf0248fbc"
      },
      "source": [
        "tfds.as_dataframe(dataset_all.take(10),info)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "</style><table id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >description</th>        <th class=\"col_heading level0 col1\" >label</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row0_col0\" class=\"data row0 col0\" >AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.</td>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row0_col1\" class=\"data row0 col1\" >3 (Sci/Tech)</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row1_col0\" class=\"data row1 col0\" >Reuters - Major League Baseball\\Monday announced a decision on the appeal filed by Chicago Cubs\\pitcher Kerry Wood regarding a suspension stemming from an\\incident earlier this season.</td>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row1_col1\" class=\"data row1 col1\" >1 (Sports)</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row2_col0\" class=\"data row2 col0\" >President Bush #39;s quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.</td>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row2_col1\" class=\"data row2 col1\" >2 (Business)</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row3_col0\" class=\"data row3 col0\" >Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.</td>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row3_col1\" class=\"data row3 col1\" >3 (Sci/Tech)</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row4_col0\" class=\"data row4 col0\" >London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.</td>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row4_col1\" class=\"data row4 col1\" >1 (Sports)</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row5_col0\" class=\"data row5 col0\" >TOKYO - Sony Corp. is banking on the \\$3 billion deal to acquire Hollywood studio Metro-Goldwyn-Mayer Inc...</td>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row5_col1\" class=\"data row5 col1\" >0 (World)</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row6_col0\" class=\"data row6 col0\" >Giant pandas may well prefer bamboo to laptops, but wireless technology is helping researchers in China in their efforts to protect the engandered animals living in the remote Wolong Nature Reserve.</td>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row6_col1\" class=\"data row6 col1\" >3 (Sci/Tech)</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row7_col0\" class=\"data row7 col0\" >VILNIUS, Lithuania - Lithuania #39;s main parties formed an alliance to try to keep a Russian-born tycoon and his populist promises out of the government in Sunday #39;s second round of parliamentary elections in this Baltic country.</td>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row7_col1\" class=\"data row7 col1\" >0 (World)</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row8_col0\" class=\"data row8 col0\" >Witnesses in the trial of a US soldier charged with abusing prisoners at Abu Ghraib have told the court that the CIA sometimes directed abuse and orders were received from military command to toughen interrogations.</td>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row8_col1\" class=\"data row8 col1\" >0 (World)</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row9_col0\" class=\"data row9 col0\" >Dan Olsen of Ponte Vedra Beach, Fla., shot a 7-under 65 Thursday to take a one-shot lead after two rounds of the PGA Tour qualifying tournament.</td>\n",
              "                        <td id=\"T_2780de40_73b5_11eb_b076_0242ac1c0002row9_col1\" class=\"data row9 col1\" >1 (Sports)</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "                                         description  label\n",
              "0  b'AMD #39;s new dual-core Opteron chip is desi...      3\n",
              "1  b'Reuters - Major League Baseball\\\\Monday anno...      1\n",
              "2  b'President Bush #39;s  quot;revenue-neutral q...      2\n",
              "3  b'Britain will run out of leading scientists u...      3\n",
              "4  b'London, England (Sports Network) - England m...      1\n",
              "5  b'TOKYO - Sony Corp. is banking on the \\\\$3 bi...      0\n",
              "6  b'Giant pandas may well prefer bamboo to lapto...      3\n",
              "7  b'VILNIUS, Lithuania - Lithuania #39;s main pa...      0\n",
              "8  b'Witnesses in the trial of a US soldier charg...      0\n",
              "9  b'Dan Olsen of Ponte Vedra Beach, Fla., shot a...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoG_cOcMMKgC"
      },
      "source": [
        "### Review Labels (Categories) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uBCsL_8tTUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a44cbb-74ed-4e27-a1c7-cd0d623cc798"
      },
      "source": [
        "print(f'There are {info.features[\"label\"].num_classes} classes in the dataset.')\n",
        "print(f'The class names are {info.features[\"label\"].names}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 4 classes in the dataset.\n",
            "The class names are ['World', 'Sports', 'Business', 'Sci/Tech']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRbULmnCtTXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5559a7d7-3119-4558-dab7-65bbc0865729"
      },
      "source": [
        "# classes dictionary\n",
        "categories =dict(enumerate(info.features[\"label\"].names))\n",
        "categories"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXQzudk9MmOs"
      },
      "source": [
        "The 127,600 news articles are evenly distributed among the 4 categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRtOaULstTbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b57fb2f3-c2e5-4768-a587-dc0d5117feb7"
      },
      "source": [
        "train_categories = [categories[label] for label in dataset_all.map(lambda text, label: label).as_numpy_iterator()]\n",
        "Counter(train_categories).most_common()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Sci/Tech', 31900), ('Sports', 31900), ('Business', 31900), ('World', 31900)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6-1-zvxdS8f"
      },
      "source": [
        "We will use the tf.keras.layers.experimental.preprocessing.TextVectorization layer to transform each news article into a \"list\" of non-negative integers representing the tokens in the news article.\n",
        "\n",
        "For the purpose of training our models each such \"encoding\" will have a fixed length corresponding to the news article(s) with the most tokens. Shorter articles will be right-padded with zeros in the encoding. Also to speed up the training process, we will set max_tokens = 1000 so that words not in the vabulary set of top 1000 most common tokes are encoded as 1. But first we set max_tokens = None (which is the default value) in order to get the vocabulary size of the corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYY7P-aaeU4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97b30f8-6ecf-4487-ff91-9a51fd6e5f61"
      },
      "source": [
        "%%time\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=None)\n",
        "encoder.adapt(dataset_all.map(lambda text, label: text))\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f01901cdae8> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f01901cdae8>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f01901cdae8> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f01901cdae8>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f01901cdae8> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f01901cdae8>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "CPU times: user 2min 4s, sys: 26.8 s, total: 2min 31s\n",
            "Wall time: 1min 34s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKGVb8_Jeyig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3835c993-99af-49ad-fffd-a13d17afec6f"
      },
      "source": [
        "print(f\"There are {len(vocab)} vocabulary words in the corpus.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 95976 vocabulary words in the corpus.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlQ3s9QUe3CX"
      },
      "source": [
        "There are 95976 vocabulary words in the corpus.\n",
        "\n",
        "The .adapt method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z2Io_yoe8zW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b68a9d-8e3d-461e-9217-a49c0ab23b79"
      },
      "source": [
        "vocab[:20]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'a', 'to', 'of', 'in', 'and', 'on', 'for',\n",
              "       'that', '39s', 'with', 'its', 'as', 'at', 'is', 'said', 'by', 'it'],\n",
              "      dtype='<U150')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg0kYN3tfEPJ"
      },
      "source": [
        "Let's use how the encoding works on a sample string all of whose words are in the vocabulary of the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE-EfC2CfIcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e29fc31-c9be-4405-aaaf-b9cc12c2f821"
      },
      "source": [
        "example = \"the dog ran after a red ball as it rolled by the hat on the ground.\"\n",
        "for word in example.split():\n",
        "  print(f'\"{word}\" is {\"*not* \" if word not in vocab  else \"\"}in the vocabulary.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"the\" is in the vocabulary.\n",
            "\"dog\" is in the vocabulary.\n",
            "\"ran\" is in the vocabulary.\n",
            "\"after\" is in the vocabulary.\n",
            "\"a\" is in the vocabulary.\n",
            "\"red\" is in the vocabulary.\n",
            "\"ball\" is in the vocabulary.\n",
            "\"as\" is in the vocabulary.\n",
            "\"it\" is in the vocabulary.\n",
            "\"rolled\" is in the vocabulary.\n",
            "\"by\" is in the vocabulary.\n",
            "\"the\" is in the vocabulary.\n",
            "\"hat\" is in the vocabulary.\n",
            "\"on\" is in the vocabulary.\n",
            "\"the\" is in the vocabulary.\n",
            "\"ground.\" is *not* in the vocabulary.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ3K5w1ZfNYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c221a0f-8344-472d-deb9-413283f49f35"
      },
      "source": [
        "encoder(example)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(16,), dtype=int64, numpy=\n",
              "array([   2, 5958, 1287,   29,    3,  232, 1414,   14,   19, 2548,   18,\n",
              "          2, 2435,    8,    2,  999])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ0MQCTAfdCL"
      },
      "source": [
        "Let us get the total number of words in the corpus and the sizes of the news articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3TlBRwMfXde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8be9046-f8a8-45db-f7dc-050c8c96219d"
      },
      "source": [
        "%%time\n",
        "doc_sizes = []\n",
        "corpus = []\n",
        "for example, _ in dataset_all.as_numpy_iterator():\n",
        "  enc_example = encoder(example)\n",
        "  doc_sizes.append(len(enc_example))\n",
        "  corpus+=list(enc_example.numpy())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 13min 49s, sys: 1min 32s, total: 15min 21s\n",
            "Wall time: 12min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEbVpRHyfXqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430396b7-01b5-474b-de4e-75160b5188da"
      },
      "source": [
        "print(f\"There are {len(corpus)} words in the corpus of {len(doc_sizes)} news articles.\")\n",
        "print(f\"Each news article has between {min(doc_sizes)} and {max(doc_sizes)} tokens in it.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 3909695 words in the corpus of 127600 news articles.\n",
            "Each news article has between 3 and 173 tokens in it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgtVVxL-i8Uo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "c19c0b14-45bd-49b5-f5cc-a5863977d5cd"
      },
      "source": [
        "plt.figure(figsize=(15,9))\n",
        "plt.hist(doc_sizes, bins=20,range = (0,120))\n",
        "plt.xlabel(\"Tokens Per Document\")\n",
        "plt.ylabel(\"Number of AG News Articles\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Number of AG News Articles')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu7roRaAjDYH"
      },
      "source": [
        "Encode the news articles using the top 1000 most common words in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4fN-QeAjCFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16406dc3-8c0a-4903-bdec-0a916908176f"
      },
      "source": [
        "%%time\n",
        "encoder_1000 = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=1000)\n",
        "encoder_1000.adapt(dataset_all.map(lambda text, label: text))\n",
        "vocab_1000 = np.array(encoder_1000.get_vocabulary())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f01278e1bf8> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f01278e1bf8>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f01278e1bf8> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f01278e1bf8>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f01278e1bf8> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f01278e1bf8>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "CPU times: user 2min 4s, sys: 26 s, total: 2min 30s\n",
            "Wall time: 1min 33s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M65FlO5QjeFb"
      },
      "source": [
        "The .adapt method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBTFJ-RUjcug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a6cb7f-34b2-4657-d886-bd81464680bb"
      },
      "source": [
        "vocab_1000[:20]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'a', 'to', 'of', 'in', 'and', 'on', 'for',\n",
              "       'that', '39s', 'with', 'its', 'as', 'at', 'is', 'said', 'by', 'it'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pejvLRAMjlZJ"
      },
      "source": [
        "In particular, 0 is use for padding, 1 for the unknown words, 2 for the common word, i.e. 'the', etc. Let us look at the same example we encoded previously using the encoder for all the vocabulary words. Note that there are now five 1's denoting words that are not in the top 1000 in frequency.\n",
        "\n",
        "We encode the same example as before using the new encoder. Note that there are now 5 out of vocabulary words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Am3_fw5jqBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b75048-73da-4f21-f232-d177803583e3"
      },
      "source": [
        "example = \"the dog ran after a red ball as it rolled by the hat on the ground.\"\n",
        "encoder_1000(example)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(16,), dtype=int64, numpy=\n",
              "array([  2,   1,   1,  29,   3, 232,   1,  14,  19,   1,  18,   2,   1,\n",
              "         8,   2, 999])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4OtDWMxjuos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c248b74-cc7b-40c5-aa65-c56b188896c6"
      },
      "source": [
        "for word in example.split():\n",
        "  print(f'\"{word}\" is {\"*not* \" if word not in vocab_1000  else \"\"}in the vocabulary.')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"the\" is in the vocabulary.\n",
            "\"dog\" is *not* in the vocabulary.\n",
            "\"ran\" is *not* in the vocabulary.\n",
            "\"after\" is in the vocabulary.\n",
            "\"a\" is in the vocabulary.\n",
            "\"red\" is in the vocabulary.\n",
            "\"ball\" is *not* in the vocabulary.\n",
            "\"as\" is in the vocabulary.\n",
            "\"it\" is in the vocabulary.\n",
            "\"rolled\" is *not* in the vocabulary.\n",
            "\"by\" is in the vocabulary.\n",
            "\"the\" is in the vocabulary.\n",
            "\"hat\" is *not* in the vocabulary.\n",
            "\"on\" is in the vocabulary.\n",
            "\"the\" is in the vocabulary.\n",
            "\"ground.\" is *not* in the vocabulary.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbrBW9_zjzOA"
      },
      "source": [
        "We want to determine the number of non-vocabulary words in each news articles (denoted by 1s in the encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoPf9hvsj5h0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f888bd6b-c3c9-41da-d26d-91ab9b75e614"
      },
      "source": [
        "%%time\n",
        "doc1000_sizes = []\n",
        "corpus1000 = []\n",
        "count1000=0\n",
        "useless = 0\n",
        "# stop = 0\n",
        "percents = []\n",
        "for example, _ in dataset_all.as_numpy_iterator():\n",
        "  # stop+=1\n",
        "  # if stop > 5: break\n",
        "  enc_example = encoder_1000(example)\n",
        "  num_ones = tf.math.count_nonzero(enc_example==1).numpy()\n",
        "  percent_ones = round(num_ones*100/len(enc_example))\n",
        "  # print(f\"{percent_ones}%\")\n",
        "  percents.append(percent_ones)\n",
        "\n",
        "  s = set(list(enc_example.numpy()))\n",
        "  if s == {1}: useless+=1\n",
        "\n",
        "  doc1000_sizes.append(len(enc_example))\n",
        "  corpus1000+=list(enc_example.numpy())\n",
        "\n",
        "  count1000 += tf.math.count_nonzero(enc_example>1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 16min 15s, sys: 1min 41s, total: 17min 57s\n",
            "Wall time: 15min 19s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vy5wpoAkHCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a0c9ca-4103-4cc1-d496-88085a13eeb7"
      },
      "source": [
        "print(f\"There are {len(corpus1000)} words in the corpus of {len(doc1000_sizes)} news articles.\")\n",
        "print(f\"Each news article has between {min(doc1000_sizes)} and {max(doc1000_sizes)} tokens in it.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 3909695 words in the corpus of 127600 news articles.\n",
            "Each news article has between 3 and 173 tokens in it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnJEjOHgkE4Y"
      },
      "source": [
        "\n",
        "\n",
        "Note below that most of the news articles consists of at least 60% (top 1000) vocabulary words (with only 22 out for 127,600 news articles containing no top 1000 vacabulary words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZFeEWE0kXE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224f6710-d893-4b12-f11a-ee0676ce0c9a"
      },
      "source": [
        "Counter(percents).most_common(10)\n",
        "np.unique(percents, return_counts=True) "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  0,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
              "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
              "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
              "         41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
              "         54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,\n",
              "         67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n",
              "         80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
              "         93,  95,  97, 100]),\n",
              " array([  12,    5,   20,   29,   49,   71,  120,  137,  172,  274,  486,\n",
              "         368,  682,  777,  828, 1346, 1376, 2123, 1610, 2863, 2687, 3018,\n",
              "        3957, 3315, 4605, 3934, 4148, 5737, 4954, 5072, 6123, 6165, 3255,\n",
              "        5095, 4596, 3438, 5880, 4205, 3109, 4063, 3555, 3030, 2904, 2294,\n",
              "        1967, 1716, 2174,  368, 2486,  234, 1246,  747,  573,  547,  520,\n",
              "         441,  348,  217,  250,  135,  234,   49,  131,  101,   11,  135,\n",
              "          40,   62,   33,   53,    8,   34,   10,   55,   10,    8,   15,\n",
              "           3,   27,    9,    5,   19,    1,    8,   12,    6,   16,   11,\n",
              "           5,    4,    2,    3,    1,    1,   22]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPJDd74Hljwz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "df665862-2487-4da8-cc65-857d2ab60d8c"
      },
      "source": [
        "plt.figure(figsize=(15,9))\n",
        "plt.hist(percents, 20)\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.xlabel('Percent of Non-Vocabulary Words in a Document');"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXb2Q-gVlo08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6afd1322-bfaa-46a8-95eb-098c2e1df7c1"
      },
      "source": [
        "print(f\"There are {len(corpus1000)} words in the corpus with {useless} documents not containing any of the top {len(vocab_1000)} vocabulary words.\")\n",
        "print(f\"There are {count1000} top {len(vocab_1000)} vocabulary words in the corpus.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 3909695 words in the corpus with 22 documents not containing any of the top 1000 vocabulary words.\n",
            "There are 2602670 top 1000 vocabulary words in the corpus.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEPOPb63RNjv"
      },
      "source": [
        "### Preprocessing Data Create Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_tU6cH0ncl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa4b36ab-a8da-4ef8-a681-fb93863a7719"
      },
      "source": [
        "# register  ag_news_subset so that tfds.load doesn't generate a checksum (mismatch) error\n",
        "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=ag_news_subset\n",
        "\n",
        "# Example Approaches to Split Data Set\n",
        "# dataset, info = tfds.load('ag_news_subset', with_info=True,  split=['train[:]','test[:1000]', 'test[1000:]'],\n",
        "dataset, info = tfds.load('ag_news_subset', with_info=True,  split=['train[:95%]','train[95%:]', 'test[:]'],\n",
        "# dataset, info = tfds.load('ag_news_subset', with_info=True,  split=['train[:114000]','train[114000:]', 'test[:]'],\n",
        "                          as_supervised=True)\n",
        "train_dataset, validation_dataset, test_dataset = dataset\n",
        "# train_dataset, test_dataset = dataset['train'],dataset['test']"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-20 20:24:13.174102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "I0220 20:24:16.083602 140367600310144 download_and_prepare.py:200] Running download_and_prepare for dataset(s):\n",
            "ag_news_subset\n",
            "I0220 20:24:16.084728 140367600310144 dataset_info.py:361] Load dataset info from /root/tensorflow_datasets/ag_news_subset/1.0.0\n",
            "I0220 20:24:16.086317 140367600310144 download_and_prepare.py:138] download_and_prepare for dataset ag_news_subset/1.0.0...\n",
            "I0220 20:24:16.086621 140367600310144 dataset_builder.py:299] Reusing dataset ag_news_subset (/root/tensorflow_datasets/ag_news_subset/1.0.0)\n",
            "\u001b[1mname: \"ag_news_subset\"\n",
            "description: \"AG is a collection of more than 1 million news articles.\\nNews articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity.\\nComeToMyHead is an academic news search engine which has been running since July, 2004.\\nThe dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc),\\ninformation retrieval (ranking, search, etc), xml, data compression, data streaming,\\nand any other non-commercial activity.\\nFor more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\\n\\nThe AG\\'s news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above.\\nIt is used as a text classification benchmark in the following paper:\\nXiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\\n\\nThe AG\\'s news topic classification dataset is constructed by choosing 4 largest classes from the original corpus.\\nEach class contains 30,000 training samples and 1,900 testing samples.\\nThe total number of training samples is 120,000 and testing 7,600.\"\n",
            "citation: \"@misc{zhang2015characterlevel,\\n    title={Character-level Convolutional Networks for Text Classification},\\n    author={Xiang Zhang and Junbo Zhao and Yann LeCun},\\n    year={2015},\\n    eprint={1509.01626},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.LG}\\n}\"\n",
            "location {\n",
            "  urls: \"https://arxiv.org/abs/1509.01626\"\n",
            "}\n",
            "splits {\n",
            "  name: \"test\"\n",
            "  shard_lengths: 7600\n",
            "  num_bytes: 2226751\n",
            "}\n",
            "splits {\n",
            "  name: \"train\"\n",
            "  shard_lengths: 120000\n",
            "  num_bytes: 35301386\n",
            "}\n",
            "supervised_keys {\n",
            "  input: \"description\"\n",
            "  output: \"label\"\n",
            "}\n",
            "version: \"1.0.0\"\n",
            "download_size: 11784327\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRc-UUujdLD6"
      },
      "source": [
        "####  Dataset Splits (Training, Test, Validation)\n",
        "#### .8934 Training, .0470 Test, .0596 Validation"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TXXMh7Hdr47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea666a33-c9d9-4a2f-90ca-9fe7d60d0369"
      },
      "source": [
        "len(train_dataset),len(validation_dataset),len(test_dataset) \n",
        "# len(train_dataset),len(test_dataset) "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(114000, 6000, 7600)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3ISb8oinPnR"
      },
      "source": [
        "### Review Distribution of Categorical Labels for the 114000 training data (news articles)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Itjr5Erroxv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70d5e95-b38a-4693-b9de-caaf74e76b77"
      },
      "source": [
        "from collections import Counter\n",
        "train_categories = [categories[label] for label in train_dataset.map(lambda text, label: label).as_numpy_iterator()]\n",
        "Counter(train_categories).most_common()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Business', 28531), ('Sports', 28495), ('World', 28491), ('Sci/Tech', 28483)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icCNaXSirCnm"
      },
      "source": [
        "Review Example with Interger Label Encoded Classification(text, label pairs):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g469sP9PYbQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26503e3d-ba89-41d9-f52a-9a371f4bf69d"
      },
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('class: ', categories[label.numpy()])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text:  b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n",
            "class:  Sci/Tech\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "#### Preprocessing Shuffle Data for Training and Create Batches of `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "validation_dataset = validation_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqkvdcFv41wC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce681a73-b9d4-493c-c963-a8875b3d2436"
      },
      "source": [
        "for example, label in train_dataset.take(2):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "texts:  [b'Nearly a decade in the making, the MBTA is entering the homestretch of a new \\\\$140 million fare collection system as officials yesterday unveiled the CharlieCard, #39; #39; a debit-type card to be used on the T in 2006.'\n",
            " b'THE GRAND NABOB of the world #39;s software giant, Microsoft, Bill Gates, has told technologists that he is not afraid of Linux. Speaking at a shindig at the Computer History Museum, Gates said that he had seen '\n",
            " b'The volume of worms and viruses is increasing, but the rate of successful attacks has dropped, according to a new report from Symantec.']\n",
            "\n",
            "labels:  [3 3 3]\n",
            "texts:  [b'Kraft, the largest US food company, on Monday will reveal details of a high-stakes marketing gamble to tie many of its lines with the popular South Beach diet.'\n",
            " b'PalmSource is to focus on wireless devices with its new version of Palm OS, but can it succeed against the likes of Microsoft and Symbian?'\n",
            " b\"AP - The Baltimore Ravens did their part to get into the playoffs. It wasn't enough. After beating the Miami Dolphins 30-23 Sunday, the Ravens had to wait until the late afternoon games ended to learn their postseason fate.\"]\n",
            "\n",
            "labels:  [2 3 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ_8Srl7Yi2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5463bbc1-e843-469e-f452-3daa15052e61"
      },
      "source": [
        "for example, label in train_dataset.take(2):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', [categories[n] for n in label.numpy()[:3]])\n",
        "  print()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "texts:  [b'By TARA BURGHART     CHICAGO (AP) -- When Rueben Martinez became a barber in the Los Angeles area, his love of literature inspired him to lend out books for customers. He noticed that few books made their way back to him...'\n",
            " b'LA CROSSE, Wis. - In 2000, political pundits summed up the race in three words: Florida, Florida, Florida...'\n",
            " b'Ford Motor Co. said Monday it will begin offering Sirius Satellite Radio as a dealer-installed option in four more vehicles by year #39;s end and is targeting up to 20 vehicle ']\n",
            "\n",
            "labels:  ['Sci/Tech', 'World', 'Business']\n",
            "\n",
            "texts:  [b'The previous NL game in an AL ballpark was in 1946, when the Boston Braves played several games at Fenway Park because a paint job on the seats at Braves Field had not dried, according to the Elias Sports Bureau.'\n",
            " b'South Korea #39;s economy may miss its 5 percent growth target for 2004 after expanding at its slowest pace in a year in the third quarter, the finance minister said.'\n",
            " b' SINGAPORE (Reuters) - Oil prices rose for the second day in  a row on Thursday with an Arctic blast forecast to hit the  United States next week, ramping up demand for heating fuels.']\n",
            "\n",
            "labels:  ['Sports', 'Business', 'Business']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "## Create the Text Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFevcItw15P_"
      },
      "source": [
        "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `experimental.preprocessing.TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
        "\n",
        "Create the layer, and pass the dataset's text to the layer's `.adapt` method:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLaLcPcXsY95"
      },
      "source": [
        "The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC25Lu1Yvuqy"
      },
      "source": [
        "VOCAB_SIZE=1000\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5tP4VcSkTvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c9efc0-5c70-463c-8aef-93e69833bbf0"
      },
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "len(vocab)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBoyjjWg0Ac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ba1d36-1ba1-42c6-b715-5c78699caea1"
      },
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'a', 'to', 'of', 'in', 'and', 'on', 'for',\n",
              "       'that', '39s', 'with', 'its', 'as', 'at', 'is', 'said', 'by', 'it'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rPfbSyoruu3"
      },
      "source": [
        "Here are the 20 least frequent words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6tWoBiwrzyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a59bdc-77c7-4f4c-aa08-0160e1661e6a"
      },
      "source": [
        "vocab[-20:]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['black', 'turn', 'build', 'countrys', 'advanced', 'whose',\n",
              "       'crisis', 'create', '23', 'sources', 'body', 'militant', 'hope',\n",
              "       'event', 'started', 'ready', 'jones', 'lawsuit', 'focus',\n",
              "       'singapore'], dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGc7C9WiwRWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb27de2-f0cb-42eb-89cc-6309a522fe18"
      },
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2,   1,   1,  87,   6,  22, 783,   1,  27,   6,   1,  68,   2,\n",
              "        288,   1,   1, 385, 203,  15,   1, 872, 220,   3,   1, 545,   8,\n",
              "          2,   1,  15,   1, 487,  67,  60,   1, 146,   4,   2,   1, 210,\n",
              "          1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [197, 626,  11, 353, 107,   1,  13, 705,  88, 272, 966,   9, 112,\n",
              "         29,   1,  15,  13,   1,   1,   6,   3,  63,   6,   2, 155, 221,\n",
              "          2, 968,  97,  17,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [999,  28,  70,  94, 240,   9,   2, 103, 121,   6,   3,   1,   8,\n",
              "         45,  12,  22,   1,   1, 814,   4, 247,   2,  72, 114,  95,  76,\n",
              "          1,  47, 443,   9,   1,   1,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cjz0bS39IN"
      },
      "source": [
        "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
        "\n",
        "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
        "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_tD0QY5wXaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06387b60-7003-45a5-d792-ef15fe57f38a"
      },
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  b'The previous NL game in an AL ballpark was in 1946, when the Boston Braves played several games at Fenway Park because a paint job on the seats at Braves Field had not dried, according to the Elias Sports Bureau.'\n",
            "Round-trip:  the [UNK] [UNK] game in an al [UNK] was in [UNK] when the boston [UNK] [UNK] several games at [UNK] park because a [UNK] job on the [UNK] at [UNK] field had not [UNK] according to the [UNK] sports [UNK]                           \n",
            "\n",
            "Original:  b'South Korea #39;s economy may miss its 5 percent growth target for 2004 after expanding at its slowest pace in a year in the third quarter, the finance minister said.'\n",
            "Round-trip:  south korea 39s economy may [UNK] its 5 percent growth target for 2004 after [UNK] at its [UNK] [UNK] in a year in the third quarter the finance minister said                                     \n",
            "\n",
            "Original:  b' SINGAPORE (Reuters) - Oil prices rose for the second day in  a row on Thursday with an Arctic blast forecast to hit the  United States next week, ramping up demand for heating fuels.'\n",
            "Round-trip:  singapore reuters oil prices rose for the second day in a [UNK] on thursday with an [UNK] [UNK] forecast to hit the united states next week [UNK] up demand for [UNK] [UNK]                                   \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zsmInBOCPO"
      },
      "source": [
        "![A drawing of the information flow in the model](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Above is a diagram of the model. \n",
        "\n",
        "1. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "1. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "2. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "3. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
        "\n",
        "  * The main advantage to a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "1. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EODAFUjNaDC"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    <b>tf.keras.layers.Bidirectional</b><br>\n",
        "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6Qzwsj5QH0S"
      },
      "source": [
        "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/BidirectionalRNN.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoHfL1ijMZm-"
      },
      "source": [
        "# Experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwfoBkmRYcP3"
      },
      "source": [
        "num_classes = 4\n",
        "model = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary())\n",
        "                              ,output_dim=32\n",
        "                                # Use masking to handle the variable sequence lengths\n",
        "                              ,mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))\n",
        "                              ,tf.keras.layers.Dense(32, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # num_classes = 4\n",
        "])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfNGE58PNaDD"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "Please note that we choose to Keras sequential model here since all the layers in the model only have single input and produce single output. </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTqE1NQXNaDD"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "<b>tf.keras.Model</b><br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
        "</div>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RztlbCugEFaI"
      },
      "source": [
        "## Compile Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNazQp3lEDv7"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>tf.keras.losses.SparseCategoricalCrossentropy</b><br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmWbBazIDe_-"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4)\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we don not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGmIGkkouUb"
      },
      "source": [
        "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "The embedding layer [uses masking](../../guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87a8-CwfKebw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7e1638-596f-4a1f-fbb2-0f3be278c54a"
      },
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False, True, True, True, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwH3nto596k"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oHB6X8i0u2H"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    <b>Module: tf.keras.callbacks</b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th1k9Dha0vCl"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>tf.keras.callbacks.EarlyStopping</b><br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping</div>\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>tf.keras.callbacks.ModelCheckpoint</b><br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw86wWS4YgR2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "0ebfa0ef-89c0-4228-b1cb-54310e34a309"
      },
      "source": [
        "%%time\n",
        "history = model.fit(train_dataset\n",
        "                    ,epochs = 5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    )"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1782/1782 [==============================] - 47s 18ms/step - loss: 1.1274 - accuracy: 0.4919 - val_loss: 0.5423 - val_accuracy: 0.8158\n",
            "Epoch 2/5\n",
            "1782/1782 [==============================] - 30s 17ms/step - loss: 0.5165 - accuracy: 0.8213 - val_loss: 0.4686 - val_accuracy: 0.8367\n",
            "Epoch 3/5\n",
            "1782/1782 [==============================] - 29s 16ms/step - loss: 0.4588 - accuracy: 0.8366 - val_loss: 0.4484 - val_accuracy: 0.8407\n",
            "Epoch 4/5\n",
            " 678/1782 [==========>...................] - ETA: 17s - loss: 0.4405 - accuracy: 0.8457"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-41d509b6d568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'history = model.fit(train_dataset\\n                    ,epochs = 5\\n                    ,validation_data=validation_dataset\\n                    )'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaNbXi43YgUT"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUZyCyI33fPU"
      },
      "source": [
        "## Plotting Performance Metrics - Single Layer Bidirectional RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFK6utDG3fjX"
      },
      "source": [
        "We use Matplotlib to create 2 plots--displaying the training and validation loss (resp. accuracy) for each (training) epoch side by side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzEOTxsc1JTJ"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VV8VwoA1Je1"
      },
      "source": [
        "history_df=pd.DataFrame(history_dict)\n",
        "history_df.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvA2u5do1JqA"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGoRjq2T1SUp"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bawulBahPZQq"
      },
      "source": [
        "## Model Architecture Summary Single Layer Bidirectional RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVd7MMCoOjMp"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o0oUFPAOjht"
      },
      "source": [
        "keras.utils.plot_model(model, \"BiDirectionalLSTM.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g1evcaRpTKm"
      },
      "source": [
        "## Stack two or more LSTM layers\n",
        "\n",
        "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "![layered_bidirectional]?raw=1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSClCrG1z8l"
      },
      "source": [
        "The interesting thing about using an `RNN` with `return_sequences=True` is that the output still has 3-axes, like the input, so it can be passed to another RNN layer, like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJTx1dsTRUtW"
      },
      "source": [
        "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/multilayerBidirectionalLSTM.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej7WDwcjM3L1"
      },
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo1jjO3vn0jo"
      },
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 32, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))\n",
        "                              ,tf.keras.layers.Dense(32, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(32, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iukhoA9vR7gx"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>tf.keras.losses.SparseCategoricalCrossentropy</b><br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zjWuJesR72x"
      },
      "source": [
        "model2.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeSE-YjdqAeN"
      },
      "source": [
        "%%time\n",
        "history2 = model2.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LdwilM1qPM3"
      },
      "source": [
        "test_loss, test_acc = model2.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x44eh6w-SjPa"
      },
      "source": [
        "## Plotting Performance Metrics - Multi-Layer Bidirectional RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YwGAxu3SjbG"
      },
      "source": [
        "We use Matplotlib to create 2 plots--displaying the training and validation loss (resp. accuracy) for each (training) epoch side by side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5Wa2bx7Suny"
      },
      "source": [
        "history_dict2 = history2.history\n",
        "history_dict2.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwzqjLbrSurM"
      },
      "source": [
        "history2_df=pd.DataFrame(history_dict2)\n",
        "history2_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CrnwjN7Suu5"
      },
      "source": [
        "losses = history2.history['loss']\n",
        "accs = history2.history['accuracy']\n",
        "val_losses = history2.history['val_loss']\n",
        "val_accs = history2.history['val_accuracy']\n",
        "epochs = len(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw20TBZxSuye"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3RlxHyPTCrD"
      },
      "source": [
        "#### Model Architecture Summary Single Layer Bidirectional RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idRIHfoySu12"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o01y6ZMVTF8J"
      },
      "source": [
        "keras.utils.plot_model(model2, \"2Layer_BiDirectionalLSTM.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xvpE3BaGw_V"
      },
      "source": [
        "Check out other existing recurrent layers such as [GRU layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU).\n",
        "\n",
        "If you're interestied in building custom RNNs, see the [Keras RNN Guide](../../guide/keras/rnn.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVBHs98CJT2X"
      },
      "source": [
        "# Experiment 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxPRgipaJZvD"
      },
      "source": [
        "model3 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))\n",
        "                              ,tf.keras.layers.Dense(64, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVQ69KKdJvgp"
      },
      "source": [
        "model3.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmBXwQRNJvp-"
      },
      "source": [
        "%%time\n",
        "history3 = model3.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quBCokPeJvwS"
      },
      "source": [
        "test_loss, test_acc = model3.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHias4K3YVbz"
      },
      "source": [
        "# Experiment 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QVCRjs4YYX-"
      },
      "source": [
        "model4 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))\n",
        "                              ,tf.keras.layers.Dense(64, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(64, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgLhP3DrYeTK"
      },
      "source": [
        "model4.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AtHR1sRYec3"
      },
      "source": [
        "%%time\n",
        "history4 = model4.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYz7FiQLYeka"
      },
      "source": [
        "test_loss, test_acc = model4.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3j5j2RRe6lU"
      },
      "source": [
        "# Experiment 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO1E0fYqe-tU"
      },
      "source": [
        "model5 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 128, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))\n",
        "                              ,tf.keras.layers.Dense(128, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDj8S4hae_BG"
      },
      "source": [
        "model5.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjJPu7lYe_Hy"
      },
      "source": [
        "%%time\n",
        "history5 = model5.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3sXMYGMe_LH"
      },
      "source": [
        "test_loss, test_acc = model5.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrqOf4hxfSLc"
      },
      "source": [
        "# Experiment 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5bzqqc_fVsj"
      },
      "source": [
        "model6 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 128, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))\n",
        "                              ,tf.keras.layers.Dense(128, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(128, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGL52C4dfV2H"
      },
      "source": [
        "model6.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtpNc2a2fV85"
      },
      "source": [
        "%%time\n",
        "history6 = model6.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUdcj5rrfWEt"
      },
      "source": [
        "test_loss, test_acc = model6.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igf0jFXofWLe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeBbZRyGiaWE"
      },
      "source": [
        "# Experiment 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIotlehJigj0"
      },
      "source": [
        "model7 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 256, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256))\n",
        "                              ,tf.keras.layers.Dense(256, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fjjYci8igxh"
      },
      "source": [
        "model7.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccVESnuXig2B"
      },
      "source": [
        "%%time\n",
        "history7 = model7.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0cYgh8Uig6Q"
      },
      "source": [
        "test_loss, test_acc = model7.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "246l18_lig9C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lczmvsfhkH4I"
      },
      "source": [
        "# Experiment 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OPsONHLkLbj"
      },
      "source": [
        "model8 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 256, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256))\n",
        "                              ,tf.keras.layers.Dense(256, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(256, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjGveiFBkLlC"
      },
      "source": [
        "model8.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eRkmSGRkLrE"
      },
      "source": [
        "%%time\n",
        "history8 = model8.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXCKvqtEkLvB"
      },
      "source": [
        "test_loss, test_acc = model8.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYac29Q4mWb7"
      },
      "source": [
        "# Experiment 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW5ERoBOkMZx"
      },
      "source": [
        "model9 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 512, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND_jtazSkMiG"
      },
      "source": [
        "model9.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC2TBIQ1mdFW"
      },
      "source": [
        "%%time\n",
        "history9 = model9.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEL9Eq50mdPL"
      },
      "source": [
        "test_loss, test_acc = model9.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WLHihlnwYT_"
      },
      "source": [
        "# Experiment 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqigMGCWwcwk"
      },
      "source": [
        "model10 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 512, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0c6l6ZJwc7j"
      },
      "source": [
        "model10.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSUlSmqiwdDq"
      },
      "source": [
        "%%time\n",
        "history10 = model10.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgxZMJ7XwdIV"
      },
      "source": [
        "test_loss, test_acc = model10.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkDc4Bf3I9mC"
      },
      "source": [
        "# Experiment 11"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8HXT7FeJChJ"
      },
      "source": [
        "model11 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 256, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "713Itve1JDKs"
      },
      "source": [
        "model11.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxZySXEWJDOZ"
      },
      "source": [
        "%%time\n",
        "history11 = model11.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK9M2kuQJDUR"
      },
      "source": [
        "test_loss, test_acc = model11.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G1X0hRMWmMo"
      },
      "source": [
        "# Experiment 12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDXlbTKYWqLc"
      },
      "source": [
        "model12 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 1024, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd03iyEEWqWH"
      },
      "source": [
        "model12.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhGdAXqSW099"
      },
      "source": [
        "%%time\n",
        "history12 = model12.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAzMoIycWqb-"
      },
      "source": [
        "test_loss, test_acc = model12.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT637zBaxoH1"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = model12.predict(test_dataset)\n",
        "predicted_categories = tf.argmax(y_pred, axis=1)\n",
        "\n",
        "\n",
        "true_categories = tf.concat([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "confusion_matrix(predicted_categories, true_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCUiQJOG78or"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "precision_score(true_categories, predicted_categories, average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwdo1XxJ782E"
      },
      "source": [
        "from sklearn.metrics import recall_score\n",
        "recall_score(true_categories, predicted_categories, average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPxWzpJ5XNZ3"
      },
      "source": [
        "# Experiment 13"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJlbYeO1XRgc"
      },
      "source": [
        "model13 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 1024, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dropout(0.5)\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_JG1sa7XRrT"
      },
      "source": [
        "model13.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n61wsafPXRxn"
      },
      "source": [
        "%%time\n",
        "history13 = model13.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suANaSQ3XR2l"
      },
      "source": [
        "test_loss, test_acc = model13.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU5MnXdSg1tn"
      },
      "source": [
        "# Experiment 14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQFwlZXhg5FY"
      },
      "source": [
        "model14 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 1024, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dropout(0.5)\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dropout(0.5)\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55G0GQplg-1W"
      },
      "source": [
        "model14.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhACQ7LPg--L"
      },
      "source": [
        "%%time\n",
        "history14 = model14.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5ucxCS9g_CK"
      },
      "source": [
        "test_loss, test_acc = model14.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ZGPPOJjB4l"
      },
      "source": [
        "# Experiment 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7aNZQ_5g_F_"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "model15 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 1024, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax', kernel_regularizer='l1')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3LLItIUyVLT"
      },
      "source": [
        "model15.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9GyG1-3zW-s"
      },
      "source": [
        "%%time\n",
        "history15 = model15.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBia3I50yVTz"
      },
      "source": [
        "test_loss, test_acc = model15.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2izpRljOjGtu"
      },
      "source": [
        "# Experiment 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "686nL7gmjJa4"
      },
      "source": [
        "model16 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 1024, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax', kernel_regularizer='l2')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZppbQw9zjJqf"
      },
      "source": [
        "model16.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHPBYYUajJ0T"
      },
      "source": [
        "%%time\n",
        "history16 = model16.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToGdyY_sF1Um"
      },
      "source": [
        "test_loss, test_acc = model16.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWppe7BPjJ-N"
      },
      "source": [
        "# Experiment 17"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlj9X1fVGXuE"
      },
      "source": [
        "model17 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 1024, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PCuCF0gGX3z"
      },
      "source": [
        "model17.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp2wlVY0GX_Y"
      },
      "source": [
        "%%time\n",
        "history17 = model17.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NKVLqH4GYGI"
      },
      "source": [
        "test_loss, test_acc = model17.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BJk5ACDSsNW"
      },
      "source": [
        "# Experiment 18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsxI4By9Sv_m"
      },
      "source": [
        "\n",
        "encoder_none = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=None)\n",
        "encoder_none.adapt(train_dataset.map(lambda text, label: text))\n",
        "model18 = tf.keras.Sequential([\n",
        "                              encoder_none\n",
        "                              ,tf.keras.layers.Embedding(len(encoder_none.get_vocabulary()), 1024, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InYQifF1SwKW"
      },
      "source": [
        "model18.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_9eqxvbSwQW"
      },
      "source": [
        "%%time\n",
        "history18 = model18.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHNmJHajSwU4"
      },
      "source": [
        "test_loss, test_acc = model18.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfHlgO71a7fd"
      },
      "source": [
        "# Experiment 19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trQzGC-0SwbB"
      },
      "source": [
        "encoder_500 = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=500)\n",
        "encoder_500.adapt(train_dataset.map(lambda text, label: text))\n",
        "model19 = tf.keras.Sequential([\n",
        "                              encoder_500\n",
        "                              ,tf.keras.layers.Embedding(len(encoder_500.get_vocabulary()), 1024, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTmssK0pbAAF"
      },
      "source": [
        "model19.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jClIMjsrbAOg"
      },
      "source": [
        "%%time\n",
        "history19 = model19.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDN2bMknbAo_"
      },
      "source": [
        "test_loss, test_acc = model19.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "171xVZl8b2PH"
      },
      "source": [
        "# Experiment 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyPpWLqtbBLn"
      },
      "source": [
        "encoder_none = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=200)\n",
        "encoder_none.adapt(train_dataset.map(lambda text, label: text))\n",
        "model20 = tf.keras.Sequential([\n",
        "                              encoder_none\n",
        "                              ,tf.keras.layers.Embedding(len(encoder_none.get_vocabulary()), 1024, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhijTecScOZm"
      },
      "source": [
        "model20.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJWbkKv5cP2w"
      },
      "source": [
        "%%time\n",
        "history20 = model20.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSK9oub5cZFj"
      },
      "source": [
        "test_loss, test_acc = model20.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_uzT0qccojT"
      },
      "source": [
        "# Experiment 21"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrf4JOp2cnPy"
      },
      "source": [
        "encoder_none = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=1000, output_sequence_length=10)\n",
        "encoder_none.adapt(train_dataset.map(lambda text, label: text))\n",
        "model21 = tf.keras.Sequential([\n",
        "                              encoder_none\n",
        "                              ,tf.keras.layers.Embedding(len(encoder_none.get_vocabulary()), 1024, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpMMA5aac6iq"
      },
      "source": [
        "model21.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AfP3vtEc6rZ"
      },
      "source": [
        "%%time\n",
        "history21 = model21.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRlZuM6ec6zb"
      },
      "source": [
        "test_loss, test_acc = model21.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1cU31QWIn9k"
      },
      "source": [
        "# Experiment 22"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTYfARBLImiY"
      },
      "source": [
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "\n",
        "model22 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(input_dim = len(encoder.get_vocabulary()), output_dim = 64, mask_zero=True)\n",
        "                              ,tf.keras.layers.Conv1D(32,7)\n",
        "                              ,tf.keras.layers.MaxPooling1D(5)\n",
        "                              ,tf.keras.layers.GlobalMaxPooling1D()\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')]) "
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFviLXdmI3VA"
      },
      "source": [
        "model22.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy()  # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0nYKvabI3eM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e8f111-286b-4fe4-995d-8e294da41997"
      },
      "source": [
        "%%time\n",
        "history22 = model22.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1782/1782 [==============================] - 16s 8ms/step - loss: 0.7222 - accuracy: 0.7272 - val_loss: 0.4113 - val_accuracy: 0.8500\n",
            "Epoch 2/5\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.3924 - accuracy: 0.8590 - val_loss: 0.3905 - val_accuracy: 0.8651\n",
            "Epoch 3/5\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.3591 - accuracy: 0.8703 - val_loss: 0.3569 - val_accuracy: 0.8734\n",
            "Epoch 4/5\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.3313 - accuracy: 0.8799 - val_loss: 0.3915 - val_accuracy: 0.8615\n",
            "Epoch 5/5\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.3063 - accuracy: 0.8896 - val_loss: 0.3502 - val_accuracy: 0.8755\n",
            "CPU times: user 1min 38s, sys: 24.6 s, total: 2min 2s\n",
            "Wall time: 58.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-hncudfI3ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06cbde5d-e32f-4e10-af5d-794486c1d6e7"
      },
      "source": [
        "test_loss, test_acc = model22.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "119/119 [==============================] - 1s 5ms/step - loss: 0.4110 - accuracy: 0.8532\n",
            "Test Loss: 0.41099876165390015\n",
            "Test Accuracy: 0.8531578779220581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxEy1BvkKLba"
      },
      "source": [
        "# Experiment 23"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io0ZcbvCI3sM"
      },
      "source": [
        "model23 = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 1024, mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(512,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(512, return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(512))\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(512, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(num_classes,activation='softmax')   # not binary since there are num_classes categories\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdOojf40LQBm"
      },
      "source": [
        "model23.compile(optimizer='adam'\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we do not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt_PxzTNLQLo"
      },
      "source": [
        "%%time\n",
        "history23 = model23.fit(train_dataset\n",
        "                    ,epochs=5\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,validation_steps=30\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzN2kvGLLQRk"
      },
      "source": [
        "test_loss, test_acc = model23.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}