# -*- coding: utf-8 -*-
"""MSDS_422_GURJUS_SINGH_ASSIGNMENT#6_Neural Networks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gu5YdToe-jVoeB3DNwjsuelgviv79-wj

# Appendix
"""

# Helper libraries
import datetime
import time
import pydot_ng as pydot
from packaging import version
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler

from collections import Counter
import numpy as np
import pandas as pd

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from tensorflow import keras
from tensorflow.keras import models
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.datasets import mnist
from plot_keras_history import plot_history

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
np.set_printoptions(precision=3, suppress=True)

print("This notebook requires TensorFlow 2.0 or above")
print("TensorFlow version: ", tf.__version__)
assert version.parse(tf.__version__).release[0] >=2

print("Keras version: ", keras.__version__)

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

(x_train, y_train), (x_test, y_test)= tf.keras.datasets.mnist.load_data()

print('x_train:\t{}'.format(x_train.shape))
print('y_train:\t{}'.format(y_train.shape))
print('x_test:\t\t{}'.format(x_test.shape))
print('y_test:\t\t{}'.format(y_test.shape))

print("First ten labels training dataset:\n {}\n".format(y_train[0:10]))

x_train

Counter(y_train).most_common()

Counter(y_test).most_common()

fig = plt.figure(figsize = (15, 9))

for i in range(50):
    plt.subplot(5, 10, 1+i)
    plt.title(y_train[i])
    plt.xticks([])
    plt.yticks([])
    plt.imshow(x_train[i].reshape(28,28), cmap='binary')

y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)

print("First ten entries of y_train:\n {}\n".format(y_train[0:10]))
print("First ten rows of one-hot y_train:\n {}".format(y_train_encoded[0:10,]))

print('y_train_encoded shape: ', y_train_encoded.shape)
print('y_test_encoded shape: ', y_test_encoded.shape)

print('x_train:\t{}'.format(x_train.shape))
print('x_test:\t\t{}'.format(x_test.shape))

x_train_reshaped = np.reshape(x_train, (60000, 784))
x_test_reshaped = np.reshape(x_test, (10000, 784))

print('x_train_reshaped shape: ', x_train_reshaped.shape)
print('x_test_reshaped shape: ', x_test_reshaped.shape)

print(set(x_train_reshaped[0]))

np.set_printoptions(linewidth=np.inf)
print("{}".format(x_train[2020]))

x_train_norm = x_train_reshaped.astype('float32') / 255
x_test_norm = x_test_reshaped.astype('float32') / 255

print(set(x_train_norm[0]))

"""# MODEL 1"""

model = Sequential([
    Dense(input_shape=[784], units = 128, activation = tf.nn.relu),
    Dense(name = "output_layer", units = 10, activation = tf.nn.softmax)
])

model.summary()

keras.utils.plot_model(model, "mnist_model.png", show_shapes=True)

model.compile(optimizer='rmsprop',           
               loss = 'categorical_crossentropy',
               metrics=['accuracy'])

t1 = time.time()
historytrain = model.fit(
    x_train_norm,
    y_train_encoded,
    epochs = 10,
    validation_split=0.20 
    )
time1 = time.time() - t1

loss, accuracy = model.evaluate(x_test_norm, y_test_encoded)
print('test set accuracy: ', accuracy * 100)

trainloss, trainaccuracy = model.evaluate(x_train_norm, y_train_encoded)
print('train set accuracy: ', trainaccuracy * 100)

preds = model.predict(x_test_norm)
print('shape of preds: ', preds.shape)

plt.figure(figsize = (12, 12))
start_index = 2020

for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    pred = np.argmax(preds[start_index + i])
    actual = np.argmax(y_test_encoded[start_index + i])
    col = 'g'
    if pred != actual:
        col = 'r'
    plt.xlabel('i={} | pred={} | true={}'.format(start_index + i, pred, actual), color = col)
    plt.imshow(x_test[start_index + i], cmap='binary')
plt.show()

"""
Enter the index value in place of the value 17 below for the prediction
that you want to plot the probability scores for
"""
index = 2042

plt.plot(preds[index])
plt.show()

history_dict = historytrain.history
history_dict.keys()

acctrain = historytrain.history['accuracy']
val_acctrain = historytrain.history['val_accuracy']
losstrain = historytrain.history['loss']
val_losstrain = historytrain.history['val_loss']

plt.plot(range(1, len(acctrain) + 1), historytrain.history['accuracy'], label = 'Training')
plt.plot(range(1, len(val_acctrain) + 1), historytrain.history['val_accuracy'], label = 'Validation')
plt.ylim([0.95, 1.0])
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Get the predicted classes:
# pred_classes = model.predict_classes(x_train_norm)# give deprecation warning
pred_classes = np.argmax(model.predict(x_train_norm), axis=-1)
pred_classes

conf_mx = tf.math.confusion_matrix(y_train, pred_classes)
conf_mx

def plot_confusion_matrix(matrix):
    """If you prefer color and a colorbar"""
    fig = plt.figure(figsize=(8,8))
    ax = fig.add_subplot(111)
    cax = ax.matshow(matrix)
    fig.colorbar(cax)

plt.matshow(conf_mx, cmap=plt.cm.gray)
plt.xlabel("Predicted Classes")
plt.ylabel("Actual Classes")
plt.show()

def plot_digits(instances, pos, images_per_row=5, **options):
    size = 28
    images_per_row = min(len(instances), images_per_row)
    images = [instance.reshape(size,size) for instance in instances]
    n_rows = (len(instances) - 1) // images_per_row + 1
    row_images = []
    n_empty = n_rows * images_per_row - len(instances)
    images.append(np.zeros((size, size * n_empty)))
    for row in range(n_rows):
        rimages = images[row * images_per_row : (row + 1) * images_per_row]
        row_images.append(np.concatenate(rimages, axis=1))
    image = np.concatenate(row_images, axis=0)
    pos.imshow(image, cmap = 'binary', **options)
    pos.axis("off")

cl_a, cl_b = 4, 9
X_aa = x_train_norm[(y_train == cl_a) & (pred_classes == cl_a)]
X_ab = x_train_norm[(y_train == cl_a) & (pred_classes == cl_b)]
X_ba = x_train_norm[(y_train == cl_b) & (pred_classes == cl_a)]
X_bb = x_train_norm[(y_train == cl_b) & (pred_classes == cl_b)]

plt.figure(figsize=(6,6))

p1 = plt.subplot(221)
p2 = plt.subplot(222)
p3 = plt.subplot(223)
p4 = plt.subplot(224)

plot_digits(X_aa[:25], p1, images_per_row=5);
plot_digits(X_ab[:25], p2, images_per_row=5);
plot_digits(X_ba[:25], p3, images_per_row=5);  
plot_digits(X_bb[:25], p4, images_per_row=5);


p1.set_title(f"{cl_a}'s classified as {cl_a}'s")
p2.set_title(f"{cl_a}'s classified as {cl_b}'s")
p3.set_title(f"{cl_b}'s classified as {cl_a}'s")
p4.set_title(f"{cl_b}'s classified as {cl_b}'s")

# plt.savefig("error_analysis_digits_plot_EXP1_valid")

plt.show()

"""# MODEL 2"""

model2 = Sequential([
    Dense(input_shape=[784], units = 128, activation = tf.nn.relu),
    Dense(name = "Hidden_Layer_1", units = 128, activation = tf.nn.relu ),
    Dense(name = "output_layer", units = 10, activation = tf.nn.softmax)
])

model2.summary()

keras.utils.plot_model(model2, "mnist_model.png", show_shapes=True)

model2.compile(optimizer='rmsprop',           
               loss = 'categorical_crossentropy',
               metrics=['accuracy'])

t1 = time.time()
history2train = model2.fit(
    x_train_norm,
    y_train_encoded,
    epochs = 10,
    validation_split=0.20 
    )
time2 = time.time() -t1

acctrain = history2train.history['accuracy']
val_acctrain = history2train.history['val_accuracy']
losstrain = history2train.history['loss']
val_losstrain = history2train.history['val_loss']

plt.plot(range(1, len(acctrain) + 1), history2train.history['accuracy'], label = 'Training')
plt.plot(range(1, len(val_acctrain) + 1), history2train.history['val_accuracy'], label = 'Validation')
plt.ylim([0.95, 1.0])
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

pred_classes = np.argmax(model2.predict(x_test_norm), axis=-1)
pred_classes

conf_mx = tf.math.confusion_matrix(y_test, pred_classes)
conf_mx

loss2, accuracy2 = model2.evaluate(x_test_norm, y_test_encoded)
print('test set accuracy: ', accuracy2 * 100)

trainloss2, trainaccuracy2 = model2.evaluate(x_train_norm, y_train_encoded)
print('train set accuracy: ', trainaccuracy2 * 100)

"""# MODEL 3"""

model3 = Sequential([
    Dense(input_shape=[784], units = 64, activation = tf.nn.relu),
    Dense(name = "Hidden_Layer_1", units = 64, activation = tf.nn.relu ),
    Dense(name = "output_layer", units = 10, activation = tf.nn.softmax)
])

model3.summary()

keras.utils.plot_model(model3, "mnist_model.png", show_shapes=True)

model3.compile(optimizer='rmsprop',           
               loss = 'categorical_crossentropy',
               metrics=['accuracy'])

t1 = time.time()
history3train = model3.fit(
    x_train_norm,
    y_train_encoded,
    epochs = 10,
    validation_split=0.20 
    )
time3 = time.time() - t1

acctrain = history3train.history['accuracy']
val_acctrain = history3train.history['val_accuracy']
losstrain = history3train.history['loss']
val_losstrain = history3train.history['val_loss']

plt.plot(range(1, len(acctrain) + 1), history3train.history['accuracy'], label = 'Training')
plt.plot(range(1, len(val_acctrain) + 1), history3train.history['val_accuracy'], label = 'Validation')
plt.ylim([0.95, 1.0])
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

loss3, accuracy3 = model3.evaluate(x_test_norm, y_test_encoded)
print('test set accuracy: ', accuracy3 * 100)

trainloss3, trainaccuracy3 = model3.evaluate(x_train_norm, y_train_encoded)
print('train set accuracy: ', trainaccuracy3 * 100)

pred_classes = np.argmax(model3.predict(x_test_norm), axis=-1)
pred_classes

conf_mx = tf.math.confusion_matrix(y_test, pred_classes)
conf_mx

"""# Model 4"""

model4 = Sequential([
    Dense(input_shape=[784], units = 64, activation = tf.nn.relu),
    Dense(name = "output_layer", units = 10, activation = tf.nn.softmax)
])

model4.summary()

keras.utils.plot_model(model4, "mnist_model.png", show_shapes=True)

model4.compile(optimizer='rmsprop',           
               loss = 'categorical_crossentropy',
               metrics=['accuracy'])

t1 = time.time()
history4train = model4.fit(
    x_train_norm,
    y_train_encoded,
    epochs = 10,
    validation_split=0.20 
    )
time4 = time.time() - t1

acctrain = history4train.history['accuracy']
val_acc = history4train.history['val_accuracy']
loss = history4train.history['loss']
val_loss = history4train.history['val_loss']

plt.plot(range(1, len(acctrain) + 1), history4train.history['accuracy'], label = 'Training')
plt.plot(range(1, len(val_acctrain) + 1), history4train.history['val_accuracy'], label = 'Validation')
plt.ylim([0.95, 1.0])
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

loss4, accuracy4 = model4.evaluate(x_test_norm, y_test_encoded)
print('test set accuracy: ', accuracy4 * 100)

trainloss4, trainaccuracy4 = model4.evaluate(x_train_norm, y_train_encoded)
print('train set accuracy: ', trainaccuracy4 * 100)

pred_classes = np.argmax(model4.predict(x_test_norm), axis=-1)
pred_classes

conf_mx = tf.math.confusion_matrix(y_test, pred_classes)
conf_mx

"""# MODEL 5"""

model5 = Sequential([
    Dense(input_shape=[784], units = 256, activation = tf.nn.relu),
    Dense(name = "output_layer", units = 10, activation = tf.nn.softmax)
])

model5.compile(optimizer='rmsprop',           
               loss = 'categorical_crossentropy',
               metrics=['accuracy'])

t1 = time.time()
history5train = model5.fit(
    x_train_norm,
    y_train_encoded,
    epochs = 10,
    validation_split=0.20 
    )
time5 = time.time() - t1

acctrain = history5train.history['accuracy']
val_acctrain = history5train.history['val_accuracy']
losstrain = history5train.history['loss']
val_losstrain = history5train.history['val_loss']

plt.plot(range(1, len(acctrain) + 1), history5train.history['accuracy'], label = 'Training')
plt.plot(range(1, len(val_acctrain) + 1), history5train.history['val_accuracy'], label = 'Validation')
plt.ylim([0.95, 1.0])
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

loss5, accuracy5 = model5.evaluate(x_test_norm, y_test_encoded)
print('test set accuracy: ', accuracy5 * 100)

trainloss5, trainaccuracy5 = model5.evaluate(x_train_norm, y_train_encoded)
print('train set accuracy: ', trainaccuracy5 * 100)

pred_classes = np.argmax(model5.predict(x_test_norm), axis=-1)
pred_classes

conf_mx = tf.math.confusion_matrix(y_test, pred_classes)
conf_mx

"""# MODEL 6"""

model6 = Sequential([
    Dense(input_shape=[784], units = 256, activation = tf.nn.relu),
    Dense(name = "Hidden_Layer_1", units = 256, activation = tf.nn.relu ),
    Dense(name = "output_layer", units = 10, activation = tf.nn.softmax)
])

model6.compile(optimizer='rmsprop',           
               loss = 'categorical_crossentropy',
               metrics=['accuracy'])

t1 = time.time()
history6 = model6.fit(
    x_train_norm,
    y_train_encoded,
    epochs = 10,
    validation_split=0.20 
    )
time6 = time.time() - t1

acctrain = history6.history['accuracy']
val_acctrain = history6.history['val_accuracy']
losstrain = history6.history['loss']
val_losstrain = history6.history['val_loss']

plt.plot(range(1, len(acctrain) + 1), history6.history['accuracy'], label = 'Training')
plt.plot(range(1, len(val_acctrain) + 1), history6.history['val_accuracy'], label = 'Validation')
plt.ylim([0.95, 1.0])
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

loss6, accuracy6 = model6.evaluate(x_test_norm, y_test_encoded)
print('test set accuracy: ', accuracy6 * 100)

trainloss6, trainaccuracy6 = model6.evaluate(x_train_norm, y_train_encoded)
print('train set accuracy: ', trainaccuracy6 * 100)

pred_classes = np.argmax(model6.predict(x_test_norm), axis=-1)
pred_classes

conf_mx = tf.math.confusion_matrix(y_test, pred_classes)
conf_mx

Experiments = [1, 2, 3, 4, 5, 6]
training  = [trainaccuracy, trainaccuracy2, trainaccuracy3, trainaccuracy4, trainaccuracy5, trainaccuracy6]
times = [time1, time2, time3, time4, time5, time6]
Nodes = [128, 128, 64, 64, 256, 256]
Layers = [1, 2, 2, 1, 1, 2]
testaccuracy = [accuracy, accuracy2, accuracy3, accuracy4, accuracy5, accuracy6]
Results = pd.DataFrame({"Experiment": Experiments, "Processing Times": times, "Nodes Per Layer": Nodes, "Layers": Layers,  "Train Set Accuracy": training, "Test Set Accuracy": testaccuracy })
Results